xlab   = "Number of Lynx Trapped")
# Add a normal distribution
curve(dnorm(x, mean = mean(lynx), sd = sd(lynx)), #standard deviation =sd
col = "thistle4",  # Color of curve
lwd = 2,           # Line width of 2 pixels
add = TRUE)        # Superimpose on previous graph
# Add two kernel density estimators
lines(density(lynx), col = "blue", lwd = 2)
lines(density(lynx, adjust = 3), col = "purple", lwd = 2)
# Add a rug plot
rug(lynx, lwd = 2, col = "gray")
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
head(iris)
summary(iris$Species)       # Categorical variable
summary(iris$Sepal.Length)  # Quantitative variable
summary(iris)               # Entire data frame
# Clear packages
detach("package:datasets", unload = TRUE)   # For base
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, psych)
head(iris)
# Get info on package
p_help(psych)           # Opens package PDF in browser
p_help(psych, web = F)  # Opens help in R Viewer
describe(iris$Sepal.Length)  # One quantitative variable
describe(iris)               # Entire data frame
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)   # For base
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load/unload base packages manually
head(iris)
hist(iris$Petal.Length)
summary(iris$Petal.Length)
summary(iris$Species)  # Get names and n for each species
# Versicolor
hist(iris$Petal.Length[iris$Species == "versicolor"],
main = "Petal Length: Versicolor")
# Virginica
hist(iris$Petal.Length[iris$Species == "virginica"],
main = "Petal Length: Virginica")
# Setosa
hist(iris$Petal.Length[iris$Species == "setosa"],
main = "Petal Length: Setosa")
# Short petals only (all Setosa)
hist(iris$Petal.Length[iris$Petal.Length < 2],
main = "Petal Length < 2")
# Short Virginica petals only
hist(iris$Petal.Length[iris$Species == "virginica" &
iris$Petal.Length < 5.5],
main = "Petal Length: Short Virginica")
# Format: data[rows, columns]
# Leave rows or columns blank to select all
i.setosa <- iris[iris$Species == "setosa", ]
# Format: data[rows, columns]
# Leave rows or columns blank to select all
i.setosa <- iris[iris$Species == "setosa", iris$Petal.Length < 2 ]
# Format: data[rows, columns]
# Leave rows or columns blank to select all
i.setosa <- iris[iris$Species == "setosa" & iris$Petal.Length < 2 , ]
# Format: data[rows, columns]
# Leave rows or columns blank to select all
iris.setosa <- iris[iris$Species == "setosa" & iris$Petal.Length < 2 , ]
# Format: data[rows, columns]
# Leave rows or columns blank to select all
iris.setosa <- iris[iris$Species == "setosa" & iris$Petal.Length < 5.5 , ]
View(iris.setosa)
View(iris.setosa)
# Format: data[rows, columns]
# Leave rows or columns blank to select all
iris.setosa <- iris[iris$Species == "virginica" & iris$Petal.Length < 2 , ]
View(iris.setosa)
View(i.setosa)
View(iris.setosa)
head(i.setosa)
summary(i.setosa$Petal.Length)
hist(i.setosa$Petal.Length)
# Clear environment
rm(list = ls())
# Clear packages
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
n1 <- 15  # Double precision by default
n1
typeof(n1)
n2 <- 1.5
n2
typeof(n2)
c1 <- "c"
c1
typeof(c1)
c2 <- "a string of text"
c2
typeof(c2)
l1 <- TRUE
typeof(l1)
l2 <- F
l2
typeof(l2)
v1 <- c(1, 2, 3, 4, 5)
v1
is.vector(v1)
v2 <- c("a", "b", "c")
v2
is.vector(v2)
v3 <- c(TRUE, TRUE, FALSE, FALSE, TRUE)
v3
is.vector(v3)
m1 <- matrix(c(T, T, F, F, T, F), nrow = 2)
View(m1)
View(m1)
m1
m2 <- matrix(c("a", "b",
"c", "d"),
nrow = 2,
byrow = T)
View(m2)
View(m2)
m2
# Give data, then dimemensions (rows, columns, tables)
a1 <- array(c( 1:24), c(4, 3, 2))
a1
vNumeric   <- c(1, 2, 3)
vCharacter <- c("a", "b", "c")
vLogical   <- c(T, F, T)
dfa <- cbind(vNumeric, vCharacter, vLogical)
dfa  # Matrix of one data type
df <- as.data.frame(cbind(vNumeric, vCharacter, vLogical))
df  # Makes a data frame with three different data types
o1 <- c(1, 2, 3)
o2 <- c("a", "b", "c", "d")
o3 <- c(T, F, T, T, F)
list1 <- list(o1, o2, o3)
list1
list2 <- list(o1, o2, o3, list1)  # Lists within lists!
list2
(coerce1 <- c(1, "b", TRUE))
# coerce1  # Parenthese around command above make this moot
typeof(coerce1)
(coerce2 <- 5)
typeof(coerce2)
(coerce3 <- as.integer(5))
typeof(coerce3)
(coerce4 <- c("1", "2", "3"))
typeof(coerce4)
(coerce5 <- as.numeric(c("1", "2", "3")))
typeof(coerce5)
(coerce6 <- matrix(1:9, nrow= 3))
is.matrix(coerce6)
(coerce7 <- as.data.frame(matrix(1:9, nrow= 3)))
is.data.frame(coerce7)
# Clear environment
rm(list = ls())
cat("\014")  # ctrl+L
(x1 <- 1:3)
(y  <- 1:9)
# Combine variables
(df1 <- cbind.data.frame(x1, y))
source("C:/Users/Salma/Downloads/Factors.R", echo=TRUE)
(x1 <- 1:3)
(y  <- 1:9)
# Combine variables
(df1 <- cbind.data.frame(x1, y))
typeof(df1$x1)
str(df1)
(x2  <- as.factor(c(1:3)))
(df2 <- cbind.data.frame(x2, y))
typeof(df2$x2)
str(df2)
x3  <- c(1:3)
df3 <- cbind.data.frame(x3, y)
(df3$x3 <- factor(df3$x3,
levels = c(1, 2, 3)))
typeof(df3$x3)
str(df3)
x4  <- c(1:3)
df4 <- cbind.data.frame(x4, y)
df4$x4 <- factor(df4$x4,
levels = c(1, 2, 3),
labels = c("macOS", "Windows", "Linux"))
df4
typeof(df4$x4)
str(df4)
x5  <- c(1:3)
df5 <- cbind.data.frame(x5, y)
(df5$x5 <- ordered(df5$x5,
levels = c(3, 1, 2),
labels = c("No", "Maybe", "Yes")))
df5
typeof(df5$x5)
str(df5)
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
# Assigns number 0 through 10 to x1
x1 <- 0:10
x1
# Descending order
x2 <- 10:0
x2
?seq  # R help on seq
# Ascending values (duplicates 1:10)
(x3 <- seq(10))
# Specify change in values
(x4 <- seq(30, 0, by = -3))
x5 <- c(5, 4, 1, 6, 7, 2, 2, 3, 2, 8)
x5
x6 <- scan()  # After running this command, go to console
# Hit return after each number
# Hit return twice to stop
x6
?rep  # R help on rep
x7 <- rep(TRUE, 5)
x7
# Repeats set
x8 <- rep(c(TRUE, FALSE), 5)
x8
# Repeats items in set
x9 <- rep(c(TRUE, FALSE), each = 5)
x9
# Clear environment
rm(list = ls())
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, rio)
# From the official R documentation
browseURL("http://j.mp/2aFZUrJ")
# CSV
rio_csv <- import("~/Desktop/mbb.csv")
# TXT
rio_txt <- import("C:\Users\Salma\Documents\mbb.csv")
# TXT
rio_txt <- import("C:\\Users\\Salma\\Documents\\mbb.csv")
head(rio_txt)
?View
View(rio_csv)
# CSV
rio_csv <- import("C:\\Users\\Salma\\Documents\\mbb.csv")
head(rio_csv)
View(rio_csv)
# Load a spreadsheet that has been saved as tab-delimited
# text file. Need to give complete address to file. This
# command gives an error on missing data but works on
# complete data.
r_txt1 <- read.table("C:\\Users\\Salma\\Documents\\mbb.txt", header = TRUE)
# Load a spreadsheet that has been saved as tab-delimited
# text file. Need to give complete address to file. This
# command gives an error on missing data but works on
# complete data.
r_txt1 <- read.table("C:\\Users\\Salma\\Documents\\mbb.txt", header = TRUE)
# This works with missing data by specifying the separator:
# \t is for tabs, sep = "," for commas. R converts missing
# to "NA"
r_txt2 <- read.table("C:\\Users\\Salma\\Documents\\mbb.txt",
header = TRUE,
sep = "\t")
# CSV FILES
# Don't have to specify delimiters for missing data
# because CSV means "comma separated values"
trends.csv <- read.csv("C:\\Users\\Salma\\Documents\\mbb.csv", header = TRUE)
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
# Clear console
cat("\014")  # ctrl+L
library(datasets)  # Load base packages manually
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, tidyverse)
head(mtcars)
cars <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
head(cars)
# Save hierarchical clustering to "hc." This codes uses
# pipes from dplyr.
hc <- cars   %>%  # Get cars data
dist   %>%  # Compute distance/dissimilarity matrix
hclust      # Computer hierarchical clusters
plot(hc)          # Plot dendrogram
rect.hclust(hc, k = 2, border = "gray")
rect.hclust(hc, k = 3, border = "blue")
rect.hclust(hc, k = 4, border = "green4")
rect.hclust(hc, k = 5, border = "darkred")
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
# Packages I load every time; uses "pacman"
pacman::p_load(pacman, dplyr, GGally, ggplot2, ggthemes,
ggvis, httr, lubridate, plotly, rio, rmarkdown, shiny,
stringr, tidyr)
library(datasets)  # Load base packages manually
head(mtcars)
cars <- mtcars[, c(1:4, 6:7, 9:11)]  # Select variables
head(cars)
# For entire data frame ####################################
pc <- prcomp(cars,
center = TRUE,  # Centers means to 0 (optional)
scale = TRUE)   # Sets unit variance (helpful)
pc <- prcomp(~ mpg + cyl + disp + hp + wt + qsec + am +
gear + carb,
data = mtcars,
center = TRUE,
scale = TRUE)
# Get summary stats
summary(pc)
# Screeplot for number of components
plot(pc)
# Get standard deviations and rotation
pc
# See how cases load on PCs
predict(pc) %>% round(2)
# Biplot of first two components
biplot(pc)
library(datasets)  # Load base packages manually
# Use pacman to load add-on packages as desired
pacman::p_load(pacman, caret, lars, tidyverse)
head(USJudgeRatings)
data <- USJudgeRatings
# Define variable groups
x <- as.matrix(data[, -12])
y <- data[, 12]
# Using variable groups
reg1 <- lm(y ~ x) #linear model
# Or specify variables individually
reg1 <- lm(RTEN ~ CONT + INTG + DMNR + DILG + CFMG +
DECI + PREP + FAMI + ORAL + WRIT + PHYS,
data = USJudgeRatings)
# Results
reg1           # Coefficients only
summary(reg1)  # Inferential tests
anova(reg1)            # Coefficients w/inferential tests
coef(reg1)             # Coefficients (same as reg1)
confint(reg1)          # CI for coefficients
resid(reg1)            # Residuals case-by-case
hist(residuals(reg1))  # Histogram of residuals
resid(reg1)            # Residuals case-by-case
hist(residuals(reg1))  # Histogram of residuals
# Conventional stepwise regression
stepwise <- lars(x, y, type = "stepwise")
# Stagewise: Like stepwise but with better generalizability
forward <- lars(x, y, type = "forward.stagewise")
# LAR: Least Angle Regression
lar <- lars(x, y, type = "lar")
# LASSO: Least Absolute Shrinkage and Selection Operator
lasso <- lars(x, y, type = "lasso")
# Comparison of R^2 for new models
r2comp <- c(stepwise$R2[6], forward$R2[6],
lar$R2[6], lasso$R2[6]) %>%
round(2)
# Clear environment
rm(list = ls())
# Clear packages
p_unload(all)  # Remove all add-ons
# Clear packages
p_unload(all)  # Remove all add-ons
detach("package:datasets", unload = TRUE)  # For base
# Clear plots
dev.off()  # But only if there IS a plot
# Clear console
cat("\014")  # ctrl+L
housing_data <- read.csv("C:\\Users\\Salma\\Documents\\AmesHousing.csv")
head(housing_data)
housing_data <- na.omit(housing_data)
housing_data$SalePrice <- log(housing_data$SalePrice)
library(caret)
corr_matrix <- cor(housing_data[, sapply(housing_data, is.numeric)])
high_corr_features <- findCorrelation(corr_matrix, cutoff=0.75)
housing_data <- housing_data[, -high_corr_features]
set.seed(123)
train_index <- createDataPartition(housing_data$SalePrice, p=0.8, list=FALSE)
str(housing_data)
# Check for missing values
sum(is.na(housing_data$SalePrice))
housing_data <- na.omit(housing_data)
housing_df <- cbind.data.frame(housing_data$SalePrice , housing_data)
str(housing_df)
housing_data <- read.csv("C:\\Users\\Salma\\Documents\\AmesHousing.csv")
housing_data <- na.omit(housing_data)
housing_data <- read.csv("C:\\Users\\Salma\\Documents\\AmesHousing.csv")
str(housing_df)
str(housing_data)
# Check for missing values
sum(is.na(housing_data$SalePrice))
# Summary of SalePrice
summary(housing_data$SalePrice)
library(caret)
corr_matrix <- cor(housing_data[, sapply(housing_data, is.numeric)])
high_corr_features <- findCorrelation(corr_matrix, cutoff=0.75)
# Calculate the correlation matrix
corr_matrix <- cor(housing_data[, sapply(housing_data, is.numeric)], use="pairwise.complete.obs")
# Check for NA values in the correlation matrix
anyNA(corr_matrix)
knitr::opts_chunk$set(echo = TRUE)
#import libraries
require(forecast)
install.packages("forecast")
install.packages("tseries")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
#import libraries
require(forecast)
require(tseries)
setwd("C:/Users/Salma/Documents/rTest")
houseTest <- read.csv("SPCS20RPSNSA.csv")
head(houseTest)
View(housing_data)
#change name of data columns to units to convert into timeseries format to pass into the arima model
#[2] second column to simplify the name
names(houseTest)[2] <- "Units"
#create a units object to transform into time series
CSunits <- houseTest$Units
#create time series object thatstarts in jan 2000 with a monthly frequency
thouseTest <- ts(CSunits, start = c(2000, 1), frequency = 12)
#check data transformation
thouseTest
#Automatically create arima model
fit <- auto.arima(thouseTest)
#Automatically create arima model
fit <- auto.arima(thouseTest)
fit
#check accuracy
accuracy(fit)
plot(forecast(fit, 12), xlab = "Date", ylab = "Units", main = "Arima forecast for Predective housing prices")
#get table of forecasted values
pred_values <- forecast(fit, 12)
pred_values
#Check assumptions of normalcy and Aurocorrelation
qqnorm(fit$residuals)
qqline(fit$residuals)
Box.test(fit$residuals, type = "Ljung-box")
#Check assumptions of normalcy and Aurocorrelation
qqnorm(fit$residuals)
qqline(fit$residuals)
Box.test(fit$residuals, type = "Ljung-Box")
#transform time series to log scale
ltHouse <- log(thouseTest)
head(ltHouse)
#log scale for seasonal decomposition
fit2 <- stl(ltHouse, s.window = "period")
plot(fit2, main = "Seasonal Decomposition of log (House Units)")
#create a season plot
ggseasonplot(thouseTest, year.labels = TRUE , col = rainbow(20))
#ARIMA model for the log
fit3 <- auto.arima(ltHouse)
fit3
#check accuracy
fitAccuracy <- data.frame(accuracy(fit))
fitAccuracy2 <- data.frame(accuracy(fit3))
fitAccuracyFinal <- rbind(fitAccuracy, fitAccuracy2)
fitAccuracyFinal
#create a plot for the forecast of the log transform
plot(forecast(fit3, 12), xlab = "Date", ylab = "Units", main = "ARIMA forecast for Log transform")
#Get table of forecasted values
#original data
pred_values <- data.frame(forecast(fit,12))
#log transformed data
pred_values2 <- data.frame(forecast(fit3, 12))
pred_values2[,1:5] <- exp(pred_values2[,1:5])
#merge forecast predictions
mergeDF <- data.frame(Data = rownames(pred_values), original_df = pred_values$Point.Forecast, LogTransformed_df = pred_values2$Point.Forecast, Difference = round(pred_values$Point.Forecast - pred_values2$Point.Forecast, 2))
mergeDF
COVID19_line_list_data <- read.csv("~/covid_r/COVID19_line_list_data.csv")
View(COVID19_line_list_data)
rm(list=ls())
install.packages("Hmisc")
libray(Hmisc)
library(Hmisc)
data <- read.csv("~/covid_r/COVID19_line_list_data.csv")
describe(data)
data$death_dummy <- as.integer(data$death != 0)
unique(data$death_dummy)
#death rate
sum(data$death_dummy) / nrow(data)
dead = subset(data, death_dummy == 1)
alive = subset(data, death_dummy== 0)
mean(dead$age)
mean(alive$age)
mean(dead$age , na.rm =TRUE)#to remive the NA entries
mean(alive$age , na.rm = TRUE)
t.test(alive$age, dead$age, alterntive = "two.sided", conf.level = 0.95)
t.test(alive$age, dead$age, alterntive = "two.sided", conf.level = 0.99)
man = subset(data, gender == "male")
woman = subset(data, gender== "female")
mean(men$death_dummy , na.rm =TRUE)#to remove the NA entries
mean(man$death_dummy , na.rm =TRUE)#to remove the NA entries
mean(woman$death_dummy , na.rm = TRUE)
t.test(man$death_dummy, woman$death_dummy, alterntive = "two.sided", conf.level = 0.99)
